<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[linux C标准库IO缓冲区--行缓冲实现]]></title>
    <url>%2F2015%2F06%2F01%2Fline-buffer-implementation%2F</url>
    <content type="text"><![CDATA[glibc中对C标准库I/O缓冲区的实现，这篇讲行缓冲。待完善。 1 glibc中的FILE数据结构实现12//&lt;stdio.h&gt;typedef struct _IO_FILE __FILE; 12345678910111213141516171819202122232425262728293031323334353637383940414243//&lt;libio.h&gt;struct _IO_FILE &#123; int _flags; /* High-order word is _IO_MAGIC; rest is flags. */#define _IO_file_flags _flags /* The following pointers correspond to the C++ streambuf protocol. */ /* Note: Tk uses the _IO_read_ptr and _IO_read_end fields directly. */ char* _IO_read_ptr; /* Current read pointer */ char* _IO_read_end; /* End of get area. */ char* _IO_read_base; /* Start of putback+get area. */ char* _IO_write_base; /* Start of put area. */ char* _IO_write_ptr; /* Current put pointer. */ char* _IO_write_end; /* End of put area. */ char* _IO_buf_base; /* Start of reserve area. */ char* _IO_buf_end; /* End of reserve area. */ /* The following fields are used to support backing up and undo. */ char *_IO_save_base; /* Pointer to start of non-current get area. */ char *_IO_backup_base; /* Pointer to first valid character of backup area */ char *_IO_save_end; /* Pointer to end of non-current get area. */ struct _IO_marker *_markers; struct _IO_FILE *_chain; int _fileno;#if 0 int _blksize;#else int _flags2;#endif _IO_off_t _old_offset; /* This used to be _offset but it&apos;s too small. */#define __HAVE_COLUMN /* temporary */ /* 1+column number of pbase(); 0 is unknown. */ unsigned short _cur_column; signed char _vtable_offset; char _shortbuf[1]; /* char* _save_gptr; char* _save_egptr; */ _IO_lock_t *_lock;#ifdef _IO_USE_OLD_IO_FILE&#125;; 其中： 指针 意义 _IO_buf_base 指向”缓冲区” _IO_buf_end 指向”缓冲区”的末尾 _IO_read_base 指向”读缓冲区” _IO_read_end 指向”读缓冲区”的末尾 _IO_read_ptr 读缓冲区的读指针 _IO_write_base 指向”写缓冲区” _IO_write_end 指向”写缓冲区”的末尾 _IO_write_ptr “写缓冲区”的写指针 上面的定义看起来给出了3个缓冲区,实际上_IO_read_base、_IO_write_base、 _IO_buf_base都指向了同一个缓冲区。缓冲区在第一次buffered I/O操作时由库函数自动申请空间,最后由相应库函数负责释放 一个小现象引起1234int ch1;int ch2;ch1 = getchar();ch2 = getchar(); 上面这个代码,第一次调用getchar的时候，其会调用读系统调用，以让用户输入，我们输入字符d然后换行表示输入结束，getchar调用成功后，输入缓冲区如下所示： 再次调用getchar时，我们会发现我们没有输入任何东西，getchar就已反回了内容给ch2，这时因为_IO_read_ptr还没有与_IO_read_end重合，表明输入缓冲区中仍有内容可读，而该内容即为第一次输入时的结束换行符，可以发现ch2正好为10(换行符\n的ASCII码值)。此次调用后，输入缓冲区如下所示： 如果我们再加一个getchar，即代码如下： 12345int ch1;int ch2;ch1 = getchar();ch2 = getchar();ch1 = getchar(); 则运行到第三次getchar时，会调用读系统调用，新输入的内容会覆盖原内容，假设输入字符b，然后换行结束输入，则输入缓冲区如下所示: 如果输入多个字符，如hello，则他们都会被读到缓冲区，然后之后如果有调用getchar或者其他标准输入函数，则函数会直接从缓冲区中读，不再发起读系统调用。输入缓冲区如下所示： 要想清除输入缓冲区的内容，某些C库(比如MSCRT)实现了fflush(stdin)，而glibc没有实现，调用此函数，stdin缓冲区没有变化。glibc下调用rewind(stdin)也没有效果。 3 行缓冲实现3-1 stdin输入假设调用fgets(buf,n,stdin);，意欲读n-1个字符，但是，输入行中的所有数据都会被写入缓冲区（不超过缓冲区大小的前提），包括表示结束输入的换行符(如果输入Ctrl-D表示文件结尾，则不会有换行符)。 如果缓冲区中的内容（即输入的字节数）大于n-1，则_IO_read_ptr向前移动了n位(即指向缓冲区中已被用户读走的字符的下一个),下次再次调用fgets操作时,就不需要再次调用系统调用read,而是直接从_IO_read_ptr开始拷贝,当_IO_read_ptr等于_IO_read_end时，标准I/O会认为已到达文件末尾或者填满的输入缓冲区已读空，再次读则需要再次调用read。 如果缓冲区中的内容（即输入的字节数）小于n-1，则有多少读多少，读完后_IO_read_ptr指向缓冲区中已被用户读走的字符的下一个。 正常读取完毕后(即_IO_read_ptr与_IO_read_end之间没有内容了)，所有_IO_read_ptr、_IO_read_base、_IO_read_end系列指针全部指向_IO_buf_base。 总结来说，就是： _IO_read_base始终指向缓冲区的开始； _IO_read_end始终指向已从内核读入缓冲区的字符的下一个； _IO_read_ptr始终指向缓冲区中已被用户读走的字符的下一个； (_IO_read_end &lt; _IO_buf_end) &amp;&amp; (_IO_read_ptr == _IO_read_end)时则表明已到达文件末尾； (_IO_read_end = _IO_buf_end) &amp;&amp; (_IO_read_ptr == _IO_read_end)时则表明缓冲区已填满且已读完。 3-2 stdout输出标准库函数先从用户指定的地址出读数据到输出缓冲区中，读多少则_IO_write_ptr往后移多少，满足以下3个条件之一会导致输出缓冲区立即被flush： 缓冲区已满; 遇到一个换行符; 用户主动调用fflush函数; 调用标准输入函数时; 输出缓冲区flush的时候，将_IO_write_base和_IO_write_ptr之间的字符通过系统调用write写入内核，然后令_IO_write_ptr等于_IO_write_base，此时标准IO认为输出缓冲区为空。 行缓冲写的时候: _IO_write_base始终指向输出缓冲区的开始； _IO_write_end始终指向输出缓冲区的开始； _IO_write_ptr始终指向输出缓冲区中已被用户写入的字符的下一个。 4 无缓冲无缓冲时,标准I/O不对字符进行缓冲存储.典型代表是stderr,glibc中将缓冲区大小(_IO_buf_end-_IO_buf_base)设为1个字节。对无缓冲的流的每次读写操作都会引起系统调用。]]></content>
      <categories>
        <category>内核与系统编程</category>
      </categories>
      <tags>
        <tag>linux kernel</tag>
        <tag>C库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux C标准库IO缓冲区--全缓冲实现]]></title>
    <url>%2F2015%2F06%2F01%2Ffull-buffer-implementation%2F</url>
    <content type="text"><![CDATA[glibc中对C标准库I/O缓冲区的实现，这篇讲全缓冲。待完善。 全缓冲，在读空或写满标准I/O缓冲区后才进行实际的read或write系统调用操作。 1 读全缓冲读的时候: _IO_read_base始终指向缓冲区的开始； _IO_read_end始终指向已从内核读入缓冲区的字符的下一个(对全缓冲来说,buffered I/O读每次都试图都将缓冲区读满,除非文件大小不足缓冲区大小)； _IO_read_ptr始终指向缓冲区中已被用户读走的字符的下一个； (_IO_read_end &lt; _IO_buf_end) &amp;&amp; (_IO_read_ptr == _IO_read_end)时则表明已到达文件末尾； (_IO_read_end = _IO_buf_end) &amp;&amp; (_IO_read_ptr == _IO_read_end)时则表明缓冲区已填满且已读完。 2 写全缓冲写的时候: _IO_write_base始终指向缓冲区的开始； _IO_write_end始终指向缓冲区的最后一个字符的下一个； _IO_write_ptr始终指向缓冲区中已被用户写入的字符的下一个； 当_IO_write_ptr与_IO_write_end重合时，表明写缓冲区满。 当写缓冲区满时或用户主动调用fflush函数，将对输出缓冲区进行flush操作，flush的时候，将_IO_write_base和_IO_write_ptr之间的字符通过系统调用write写入内核.]]></content>
      <categories>
        <category>内核与系统编程</category>
      </categories>
      <tags>
        <tag>linux kernel</tag>
        <tag>C库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[真正理解Unix IO模型，同步/异步与阻塞/非阻塞]]></title>
    <url>%2F2015%2F05%2F21%2Funix-io-model%2F</url>
    <content type="text"><![CDATA[本文致力于理清以下几个点： 不同知识背景的人对这四个概念的具体理解是不一样的，在不同语境下也会有不一样； 当我们谈同步异步的时候，我们更侧重于说什么？当我们谈阻塞非阻塞的时候，我们更侧重于说什么？ unix环境下网络IO模型的理清； 概念首先解决前两个问题，我们从纯概念层面来看： 同步异步描述两个对象之间的关系，表示一种协作关系，不同场景下有细分区别； 而阻塞非阻塞描述一个对象的状态，就是该对象（如进程）阻在这儿了，不能继续往下执行了。阻塞可以当做拿来实现同步的一种手段。 同步异步的概念是要分场景的我们最常见的就是指编程里面的调用，同步调用与异步调用： 当发出一个同步调用后，在没有得到结果之前，该调用就不返回，而一般来说返回就得到结果。也就是必须一件一件事做，等前一件做完了才能做下一件事。 当发出一个异步调用后，会立即返回，但调用者不能立刻得到结果。实际处理这个调用的部件在完成后，通过状态、通知或回调函数来通知调用者。 一般来说，同步会用阻塞来实现（因为你既然什么事都不做，那还让你占用CPU干嘛），但同步调用也未必一定会导致阻塞(退出CPU执行队列)，例如调用者可以去死循环询问。而异步调用一般会立即返回，只是返回的不是结果，一般来说异步不会导致阻塞。 再提几个不同背景下的同步异步概念： 同步线程与异步线程： 同步线程：即两个线程步调要一致，要相互协商。 异步线程：步调不用一致，各自按各自的步调运行，不受另一个线程的影响。 同步是指两个线程的运行是相关的，其中一个线程可能要阻塞等待另外一个线程的运行；异步的意思是两个线程毫无相关，自己运行自己的。 通信领域下的概念，同步通信与异步通信： 这里的同步和异步是指：发送方和接收方是否协调步调一致！ 同步通信是指：发送方和接收方通过一定机制，实现收发步调协调。如：发送方发出数据后，等接收方发回响应以后才发下一个数据包的通讯方式 异步通信是指：发送方的发送不管接收方的接收状态，如：发送方发出数据后，不等接收方发回响应，接着发送下个数据包的通讯方式。 同步不等于阻塞，异步也不等于非阻塞 实际编程场景下的IO下面谈谈实际编程场景下的IO，内容出自史诗级巨著Richard Stevens的《UNIX网络编程·卷一》 对于一个network IO，它会涉及到两个系统对象，一个是调用这个IO的用户态进程(或线程)，另一个就是系统内核。当一个read发生时，它会经历两个阶段： 等待设备数据的准备； 系统从内核态复制数据到用户进程。 这个步骤的区分很重要，因为不同IO模型的区别就在这里。 unix环境下网络IO的五种IO Model： blocking IO non-blocking IO IO multiplexing signal driven IO asynchronous IO blocking IOlinux中，默认情况下所有的socket都是blocking。 当用户进程调用了系统调用读，kernel就开始了IO的第一个阶段：准备数据。对于network io来说，很多时候数据在一开始还没有到达（比如，还没有收到一个完整的UDP包），这个时候就要等待足够的数据到来。而在用户进程这边，整个进程会被阻塞。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 non-blocking IO在调用读系统调用时，可以通过设置参数使其变为non-blocking。 从图中可以看出，当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的系统调用，那么它马上就将数据拷贝到了用户内存，注意在拷贝数据时用户进程是阻塞的。 这里，用户进程其实是需要不断的主动询问kernel数据好了没有。 IO multiplexing就是select，poll，epoll等系统调用，有些地方也称这种IO方式为event driven IO。我们都知道，select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select/epoll这个系统调用中会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select中注册的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read系统调用，将数据从kernel拷贝到用户进程。 这个图和blocking IO的图其实并没有太大的不同，但注意这里需要使用两个系统调用，而blocking IO只调用了一个系统调用。用select的优势在于它可以同时处理多个连接。（所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） 在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 signal driven IOlinux下的signal driven IO用的很少。 首先要安装一个信号处理程序。此系统调用立即返回，用户进程继续工作，它是非阻塞的。当数据报准备好时，就生成一个SIGIO信号来通知用户进程。我们就可以立即在信号处理程序中调用recvfrom来读数据报。这种模型的好处是等待数据时可以不阻塞。 asynchronous IOinux下的asynchronous IO其实用得也很少。 用户进程发起read操作之后，内核会立即返回状态，所以不会对用户进程产生任何block，用户进程可以继续执行。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal(在aio_read调用中注册)，告诉它read操作完成了。 总结POSIX对同步IO和异步IO的定义： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes; An asynchronous I/O operation does not cause the requesting process to be blocked; 可知，同步I/O操作引起请求进程阻塞，直到I/O操作完成；异步I/O操作不引起请求进程阻塞。根据这个定义前四个模型都属于同步I/O模型，因为他们要么在第一阶段就已经阻塞要么在第二阶段开始阻塞，只有异步I/O模型才与这个异步I/O的定义相符。 这里，non-blocking IO和asynchronous IO的区别还是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而asynchronous IO则完全不同，它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。 看得出来，这里的同步异步的定义是不能推广开来的，只限于unix下的IO分类，比如wiki上就认为asynchronous IO和non-blocking IO是一个东西。 参考http://www.cnblogs.com/albert1017/p/3914149.htmlhttp://blog.chinaunix.net/uid-26000296-id-3754118.htmlhttp://blog.chinaunix.net/uid-21411227-id-1826898.htmlhttp://blog.csdn.net/historyasamirror/article/details/5778378http://blog.csdn.net/hguisu/article/details/7453390 updated 20150821]]></content>
      <categories>
        <category>内核与系统编程</category>
      </categories>
      <tags>
        <tag>network</tag>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何调用系统调用]]></title>
    <url>%2F2015%2F05%2F10%2Fsyscall%2F</url>
    <content type="text"><![CDATA[C标准库的很多IO函数都是要调用系统调用的，所以问题来了，怎么调用系统调用？ 系统调用每个函数的名称前缀都是sys_,该函数唯一标识为一个系统调用。 内核中的系统调用定义比如getpid()这个系统调用在内核的实现: 1234SYSCALL_DEFINE0(getpid)&#123; return task_tgid_vnr(current);&#125; 在内核中对应的是这样一个内核函数，这里使用了一个SYSCALL_DEFINE0，这是一个宏，0表示这是一个无参数的系统调用，同样还有SYSCALL_DEFINE1等表示含有一个参数，二个参数等的系统调用。 这个宏的实现如下: 123#define SYSCALL_DEFINE0(sname) \ SYSCALL_METADATA(_##sname, 0); \ asmlinkage long sys_##sname(void) 调用系统调用有以下几种方法调用系统调用： 通过glibc封装的函数linux系统上glibc库，已经帮我们封装了一些系统调用库函数，通过这些库函数可以直接调用系统调用。 调用syscall函数 syscall - indirect system call 如果内核新增了系统调用，但是glibc库又没有进行封装，这个时候可以使用syscall函数(其实现可参见glibc的源码,sysdeps/unix/sysv/linux/i386/syscall.S及sysdeps/unix/sysv/linux/x86_64/syscall.S)来调用系统调用，加上系统调用号和参数就可以直接调用系统调用了。示例如下： 12345678910111213#define _GNU_SOURCE /* or _BSD_SOURCE or _SVID_SOURCE */#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/syscall.h&gt; /* For SYS_xxx definitions */#include &lt;sys/types.h&gt; int main(int argc, char *argv[])&#123; pid_t tid; tid = syscall(SYS_getpid); printf("%d\n",tid); printf("%d\n",getpid()); return 0;&#125; 两种调用输出结果是一样的。 syscall函数，其在/usr/include/unistd.h下定义，该头文件是glibc编译安装后装入系统的，其声明了syscall这个函数，如下： 12345678#ifdef __USE_MISC/* Invoke `system call' number SYSNO, passing it the remaining arguments. This is completely system-dependent, and not often useful. In Unix, `syscall' sets `errno' for all errors and most calls return -1 for errors; in many systems you cannot pass arguments or get return values for all system calls (`pipe', `fork', and `getppid' typically among them). In Mach, all system calls take normal arguments and always return an error code (zero for success). */extern long int syscall (long int __sysno, ...) __THROW;#endif /* Use misc. */ 这里好奇一下，__USE_MISC是什么？另外上面示例程序中开头的_GNU_SOURCE又代表着什么？在/usr/include/unistd.h中的开头就include了/usr/include/features.h这个文件，我们进去看一下，基本上全是关于各种标准的的宏定义，首先看最前面的注释，如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/* These are defined by the user (or the compiler) to specify the desired environment: __STRICT_ANSI__ ISO Standard C. _ISOC99_SOURCE Extensions to ISO C89 from ISO C99. _POSIX_SOURCE IEEE Std 1003.1. _POSIX_C_SOURCE If ==1, like _POSIX_SOURCE; if &gt;=2 add IEEE Std 1003.2; if &gt;=199309L, add IEEE Std 1003.1b-1993; if &gt;=199506L, add IEEE Std 1003.1c-1995; if &gt;=200112L, all of IEEE 1003.1-2004 if &gt;=200809L, all of IEEE 1003.1-2008 _XOPEN_SOURCE Includes POSIX and XPG things. Set to 500 if Single Unix conformance is wanted, to 600 for the sixth revision, to 700 for the seventh revision. _XOPEN_SOURCE_EXTENDED XPG things and X/Open Unix extensions. _LARGEFILE_SOURCE Some more functions for correct standard I/O. _LARGEFILE64_SOURCE Additional functionality from LFS for large files. _FILE_OFFSET_BITS=N Select default filesystem interface. _BSD_SOURCE ISO C, POSIX, and 4.3BSD things. _SVID_SOURCE ISO C, POSIX, and SVID things. _ATFILE_SOURCE Additional *at interfaces. _GNU_SOURCE All of the above, plus GNU extensions. _REENTRANT Select additionally reentrant object. _THREAD_SAFE Same as _REENTRANT, often used by other systems. _FORTIFY_SOURCE If set to numeric value &gt; 0 additional security measures are defined, according to level. The `-ansi' switch to the GNU C compiler defines __STRICT_ANSI__. If none of these are defined, the default is to have _SVID_SOURCE, _BSD_SOURCE, and _POSIX_SOURCE set to one and _POSIX_C_SOURCE set to 200112L. If more than one of these are defined, they accumulate. For example __STRICT_ANSI__, _POSIX_SOURCE and _POSIX_C_SOURCE together give you ISO C, 1003.1, and 1003.2, but nothing else. These are defined by this file and are used by the header files to decide what to declare or define: __USE_ISOC99 Define ISO C99 things. __USE_ISOC95 Define ISO C90 AMD1 (C95) things. __USE_POSIX Define IEEE Std 1003.1 things. __USE_POSIX2 Define IEEE Std 1003.2 things. __USE_POSIX199309 Define IEEE Std 1003.1, and .1b things. __USE_POSIX199506 Define IEEE Std 1003.1, .1b, .1c and .1i things. __USE_XOPEN Define XPG things. __USE_XOPEN_EXTENDED Define X/Open Unix things. __USE_UNIX98 Define Single Unix V2 things. __USE_XOPEN2K Define XPG6 things. __USE_XOPEN2KXSI Define XPG6 XSI things. __USE_XOPEN2K8XSI Define XPG7 XSI things. __USE_LARGEFILE Define correct standard I/O things. __USE_LARGEFILE64 Define LFS things with separate names. __USE_FILE_OFFSET64 Define 64bit interface as default. __USE_BSD Define 4.3BSD things. __USE_SVID Define SVID things. __USE_MISC Define things common to BSD and System V Unix. __USE_ATFILE Define *at interfaces and AT_* constants for them. __USE_GNU Define GNU extensions. __USE_REENTRANT Define reentrant/thread-safe *_r functions. __USE_FORTIFY_LEVEL Additional security measures used, according to level. __FAVOR_BSD Favor 4.3BSD things in cases of conflict. The macros `__GNU_LIBRARY__', `__GLIBC__', and `__GLIBC_MINOR__' are defined by this file unconditionally. `__GNU_LIBRARY__' is provided only for compatibility. All new code should use the other symbols to test for features. All macros listed above as possibly being defined by this file are explicitly undefined if they are not explicitly defined. Feature-test macros that are not defined by the user or compiler but are implied by the other feature-test macros defined (or by the lack of any definitions) are defined by the file. */ 得知，该文件通过判断用户定义了哪些宏，然后主动定义相应的宏。如果定义了_GNU_SOURCE，则所有的标准都会被定义,如果什么都没定义，那么_BSD_SOURCE和_SVID_SOURCE会默认被定义。这里牵扯到UNIX的标准及各种实现分支的关系。 比如如果你定义了_BSD_SOURCE那么__USE_BSD就会定义，_SVID_SOURCE定义了则__USE_SVID会定义，前面两个只要有一个定义了，那么__USE_MISC会被定义。 实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344/* If _GNU_SOURCE was defined by the user, turn on all the other features. */#ifdef _GNU_SOURCE # undef _ISOC95_SOURCE# define _ISOC95_SOURCE 1# undef _ISOC99_SOURCE# define _ISOC99_SOURCE 1# undef _POSIX_SOURCE# define _POSIX_SOURCE 1# undef _POSIX_C_SOURCE# define _POSIX_C_SOURCE 200809L# undef _XOPEN_SOURCE# define _XOPEN_SOURCE 700# undef _XOPEN_SOURCE_EXTENDED# define _XOPEN_SOURCE_EXTENDED 1# undef _LARGEFILE64_SOURCE# define _LARGEFILE64_SOURCE 1# undef _BSD_SOURCE# define _BSD_SOURCE 1# undef _SVID_SOURCE# define _SVID_SOURCE 1# undef _ATFILE_SOURCE# define _ATFILE_SOURCE 1#endif/* If nothing (other than _GNU_SOURCE) is defined, define _BSD_SOURCE and _SVID_SOURCE. */#if (!defined __STRICT_ANSI__ &amp;&amp; !defined _ISOC99_SOURCE &amp;&amp; \ !defined _POSIX_SOURCE &amp;&amp; !defined _POSIX_C_SOURCE &amp;&amp; \ !defined _XOPEN_SOURCE &amp;&amp; !defined _BSD_SOURCE &amp;&amp; !defined _SVID_SOURCE)# define _BSD_SOURCE 1# define _SVID_SOURCE 1#endif#if defined _BSD_SOURCE || defined _SVID_SOURCE# define __USE_MISC 1#endif#ifdef _BSD_SOURCE# define __USE_BSD 1#endif#ifdef _SVID_SOURCE# define __USE_SVID 1#endif 既然挖到这一步了，何不再挖一步？ 上面示例程序中我们include了/usr/include/sys/syscall.h这个头文件，在外面一层目录还有/usr/include/syscall.h这个文件，不过这个文件的内容就是包含/usr/include/sys/syscall.h，/usr/include/sys/ssyscall.h头文件的内容如下： 1234567891011#ifndef _SYSCALL_H#define _SYSCALL_H 1/* This file should list the numbers of the system the system knows. But instead of duplicating this we use the information available from the kernel sources. */#include &lt;asm/unistd.h&gt;#ifndef _LIBC/* The Linux kernel header file defines macros `__NR_&lt;name&gt;', but some programs expect the traditional form `SYS_&lt;name&gt;'. So in building libc we scan the kernel's list and produce &lt;bits/syscall.h&gt; with macros for all the `SYS_' names. */#include &lt;bits/syscall.h&gt;#endif 其中/usr/include/bits/syscall.h文件是glibc编译过程中扫描内核系统调用列表后生成的，其将SYS_&lt;name&gt;宏转换成内核头文件使用的__NR_&lt;name&gt;宏，用户程序中不要直接include该文件，而应该include/usr/include/sys/syscall.h。 而/usr/include/asm/unistd.h则是定义了所有的系统调用号，该文件的内容很少，就是分别include /usr/include/asm/unistd_32.h和/usr/include/asm/unistd_64.h两种文件，前者定义的以i386为代表的x86-32体系结构下linux的系统调用号，而后者则定义的是x86-64体系结构下linux的系统调用号，注意两种系统调用号系统是不一致的。 直接汇编调用这是最原始的方法，上面的syscall函数即代替我们做了这样的事，其实现中会依据情况去调用下面说的三种指令：int $0x80、sysenter和syscall。这篇linux下系统调用的实现详细讲解了这一部分内容。基本的x86体系下系统调用相关的指令可以看阅读笔记：x86系统调用入门. 以gcc为例，在C语言中插入汇编，需要了解gcc inline asm的相关使用语法。其一般语法为： 1__asm__ __volatile__ ("asm statements" : output : input : modify); GCC Inline ASM 2.6.3 Gcc嵌入式汇编 当然，还得了解系统调用的过程，到底发生了什么，这里暂不叙述。可参考： Linux系统调用过程研究探究 SCI 并添加自己的调用、Explore the SCI and add your own callsLinux系统调用 int指令这是老版本linux内核中引起系统调用的唯一方式，用户态程序通过软中断指令int 0x80来陷入内核态，这是一个多路汇聚以及分解的过程，该汇聚点就是0x80中断这个入口点（X86系统结构），所有系统调用都从用户空间中汇聚到0x80中断点，当0x80中断处理程序(system_call函数，这个函数和硬件体系结构密切相关，在x86-64系统中是在entry_64.S文件中定义的)运行时，将根据系统调用号调用不同的内核函数处理。参数的传递是通过寄存器，eax传递的是系统调用号，ebx、ecx、edx、esi和edi来依次传递最多五个参数，当系统调用返回时，返回值存放在 eax 中。示例程序： 1234567891011121314#define _GNU_SOURCE /* or _BSD_SOURCE or _SVID_SOURCE */#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/syscall.h&gt; /* For SYS_xxx definitions */#include &lt;sys/types.h&gt; int main(int argc, char *argv[])&#123; pid_t tid; //tid = syscall(SYS_getpid); __asm__ volatile ("int $0x80":"=a"(tid):"a"(SYS_getpid)); printf("%d\n",tid); printf("%d\n",getpid()); return 0;&#125; getpid这个系统调用由于没有参数，所以该内联汇编会比较简单，稍复杂的可以参考相关语法。 需要编译生成32位可执行文件，这在64位系统下可能需要额外再安装32位的GNU C Library及libx32gcc开发包，比如在Ubuntu下可以输入apt-get install libc6-dev-i386来安装，其会依赖安装相关包。 sysenter指令Intel Pentium II中引入了sysenter指令，比int指令更快，更专业的陷入内核执行系统调用。 syscall指令将上面的示例程序修改一下，如下所示： 1234567891011121314#define _GNU_SOURCE /* or _BSD_SOURCE or _SVID_SOURCE */#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/syscall.h&gt; /* For SYS_xxx definitions */#include &lt;sys/types.h&gt; int main(int argc, char *argv[])&#123; pid_t tid; //tid = syscall(SYS_getpid); __asm__ volatile ("syscall":"=a"(tid):"a"(SYS_getpid)); printf("%d\n",tid); printf("%d\n",getpid()); return 0;&#125;]]></content>
      <categories>
        <category>内核与系统编程</category>
      </categories>
      <tags>
        <tag>linux kernel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux C标准库函数使用]]></title>
    <url>%2F2015%2F05%2F01%2Ffile-access-and-IO-buffer%2F</url>
    <content type="text"><![CDATA[相关头文件 stdio.h FILE fopen ( const char filename, const char mode );int fclose ( FILE stream );FILE freopen ( const char filename, const char mode, FILE stream );void setbuf ( FILE stream, char buffer );int setvbuf ( FILE stream, char buffer, int mode, size_t size );int fflush ( FILE * stream ); 1 两类IO Unbuffered I/O，是操作系统内核部分，也是系统调用。因为这里的不带缓冲指用户空间没有缓冲，即在用户的进程中对这两个函数不会缓冲，内核空间是有缓存的； Buffered I/O，严格来讲应该叫User buffered IO ， C库里面的这些IO也叫Standard IO。这里的缓冲是在用户层再建立一个缓冲区，这个缓冲区的分配和优化长度等细节都是标准IO库代你处理，不依赖系统内核，所以移植性强，一般是为了效率考虑对这些系统调用的封装。目的是为了减少system calls次数以及block-align I/O operations。比如在写的时候，可以先把数据只写在用户空间缓冲区，当设定阈值(ideally, the underlying filesystem’s block size or an integer multiple thereof)达到时，再调用write系统调用。 Unix I/O函数都是针对文件描述符，对于C标准I/O库来说，打开的文件由流(FILE *指针)标识。当打开一个流时，使一个流与文件相结合，标准I/O函数fopen返回一个指向FILE对象的指针。该对象通常是一个结构，它包含了I/O库为管理该流所需要的所有信息：用于实际I/O的文件描述符，指向流缓冲的指针，缓冲区的长度，当前在缓冲区中的字符数，出错标志等等。 1.1 磁盘文件IO过程当应用程序尝试读取某块数据的时候，如果这块数据已经存放在页缓存(内核高速缓冲)中，那么这块数据就可以立即返回给应用程序，而不需要经过实际的物理读盘操作。当然，如果数据在应用程序读取之前并未被存放在页缓存中，那么就需要先将数据从磁盘读到页缓存中去。 对于写操作来说，应用程序也会将数据先写到页缓存中去（如果是调用标准库I/O进行写，那么首先是写到标准库的缓冲区内，如果标准库的缓冲区写满以后，再写到页缓冲内；如果是系统调用，那么直接写到页缓冲内）。 处于页缓存中的数据是否被立即写到磁盘上去取决于应用程序所采用的写操作机制：如果用户采用的是同步写机制,那么数据会立即被写回到磁盘上，应用程序会一直等到数据被写完为止；如果用户采用的是延迟写机制，那么应用程序就完全不需要等到数据全部被写回到磁盘，数据只要被写到页缓存中去就可以了。在延迟写机制的情况下，操作系统会定期地将放在页缓存中的数据刷到磁盘上。与异步写机制不同的是，延迟写机制在数据完全写到磁盘上得时候不会通知应用程序，而异步写机制在数据完全写到磁盘上得时候是会返回给应用程序的。所以延迟写机制本身是存在数据丢失的风险的，而异步写机制则不会有这方面的担心。 无缓冲IO操作数据流向路径：数据——内核缓存区——磁盘 标准IO操作数据流向路径：数据——流缓冲区——内核缓存区——磁盘 访问大量小数据时，标准IO会占优，而访问数据量较大的文件时，无缓存IO会占优。 2 标准库IO缓冲区用户程序调用C标准I/O库函数读写文件或设备，而这些库函数要通过系统调用把读写请求传给内核，最终由内核驱动磁盘或设备完成I/O操作。C标准库为每个打开的文件分配一个I/O缓冲区以加速读写操作，通过文件的FILE结构体可以找到这个缓冲区，这样可以让用户调用读写函数大多数时候都在I/O缓冲区中读写，只有少数时候需要把读写请求传给内核。所以，标准I/O库提供缓冲的目的是减少read和write调用次数，节省执行I/O所需的CPU时间。 以fgetc/fputc为例，当用户程序第一次调用fgetc读一个字节时，fgetc函数可能通过系统调用进入内核读1K字节(如果文件有这么大)到I/O缓冲区中，然后返回I/O缓冲区中的第一个字节给用户，把读写位置指向I/O缓冲区中的第二个字符，以后用户再调fgetc，就直接从I/O缓冲区中读取，而不需要进内核了，当用户把这1K字节都读完之后，再次调用fgetc时，fgetc函数会再次进入内核读1K字节到I/O缓冲区中。 C标准库之所以会从内核预读一些数据放在I/O缓冲区中，是希望用户程序随后要用到这些数据，C标准库的I/O缓冲区也在用户空间，直接从用户空间读取数据比进内核读数据要快得多，C标准库和内核之间的关系就像在“Memory Hierarchy”中CPU、Cache和内存之间的关系一样。 另一方面，用户程序调用fputc通常只是写到I/O缓冲区中，这样fputc函数可以很快地返回，如果I/O缓冲区写满了，fputc就通过系统调用把I/O缓冲区中的数据传给内核，内核最终把数据写到磁盘。I/O缓冲区中的数据除了由标准I/O库函数自动冲洗外，也可手动立即强制冲洗，对应的库函数是fflush。 fclose函数在关闭文件之前也会做flush操作，它会冲洗输出缓冲的输出数据，丢弃输入缓冲的输入数据。当一个进程正常终止时，比如调用exit(), 或从main返回(main 函数被启动代码这样调用exit(main(argc, argv));)，exit()会执行标准I/O库的清理关闭操作：为所有打开流调用fclose(),然后通过_exit系统调用进入内核退出当前进程。 2.1 缓冲区类型C标准库的I/O缓冲区有三种类型：全缓冲、行缓冲和无缓冲。当用户程序调用库函数做写操作时，不同类型的缓冲区具有不同的特性。 全缓冲：如果缓冲区满了就写回内核(调用write系统调用)。常规文件(磁盘上的文件)通常是全缓冲的。 行缓冲：如果用户程序写的数据中有换行符或者缓冲区写满了就写回内核。标准输入和标准输出对应终端设备时通常是行缓冲的。 无缓冲：用户程序每次调库函数做写操作都要通过系统调用写回内核。标准错误输出通常是无缓冲的，这样用户程序产生的错误信息可以尽快输出到设备。 对任何一个给定的流，如果我们并不喜欢这些系统默认的情况，则可调用下列两个函数中的一个更改缓冲类型： 123void setbuf( FILE *restrict fp, char *restrict buf );int setvbuf( FILE *restrict fp, char *restrict buf, int mode, size_t size );//返回值：若成功则返回0，若出错则返回非0值 上表中，合适长度指的是由stat结构中的成员st_blksize所指定，如果系统不能为该流决定此值（例如若此流涉及一个设备或一个管道），则分配长度为BUFSIZ(一般为1024)的缓存。 fflush 原型：int fflush ( FILE * stream ); 功能： 如果stream被打开来写(或者update打开，并且最近一次IO是输出操作)，那此函数将输出缓冲区的任何未写入数据写入文件（严格来讲，是写入内存缓存区高速缓存）。 如果stream指向输入流（如stdin），那么fflush函数的行为是不确定的。故而使用 fflush(stdin)是不正确的，在有些库实现中，冲刷正在读的stream会导致输入缓冲区内容被丢弃。由于标准未规定，所以这不是一个可移植行为。在其它所有标准未规定的情况下，该调用行为取决于相应库的实现。见示例2。 如果stream参数是null，那么会对所有打开文件的IO缓冲区做flush操作； 该函数调用之后stream仍然保持打开； 当一个流关闭后，比如调用fclose，或者程序正常终止(因为exit()会执行标准I/O库的清理关闭操作，即为所有打开流调用fclose())了，文件的所有缓冲都会被自动冲刷，而且还会调用fsync确保将其写到磁盘上。 参数：stream：Pointer to a FILE object that identifies the stream. 返回值：成功则返回零，如果发生错误，则返回EOF， error indicator也会被设置。 使用： 无论当前采用何种缓冲区模式，在任何时候都可以使用fflush库函数强制将stream输出缓冲区的数据刷新到内核缓冲区，注意与fsync区分，详细分析见后文I/O缓冲区探讨。 在包括glibc库在内的许多C函数库实现中，若stdin和stdout指向同一终端，那么无论何时从stdin中读取输入时，都将隐含调用一次fflush(stdout)函数,这将刷新写入stdout的任何提示。当然SUSv3和C99并未规定这个行为，所以有的C库没有实现这点，要保证程序的可移植性最好显式调用fflush(stdout)。 若打开一个流同时用于输入和输出，则C99标准提出了两个要求。首先，一个输出操作后不能紧跟一个输入操作，必须在二者之间调用fflush函数或是一个文件定位函数(fseek,fsetpos,rewind等)。其次，一个输入操作后不能紧跟一个输出操作，必须在二者之间调用一个文件定位函数，除非输入操作遭遇文件结尾。 示例 1： 当文件以update (既可读又可写)方式打开,output操作之后input之前，stream必须被冲刷。可以调用repositioning相关的调用(fseek, fsetpos, rewind等)或者调用精确的fflush： 123456789101112131415161718192021/*******************************************************************Author: luowei***Description: none****************************************************************/#include &lt;stdio.h&gt;char mybuffer[80];int main()&#123; FILE * fp; fp = fopen ("test.txt","r+"); if (fp == NULL) perror ("Error opening file"); else &#123; fputs ("test",fp); //fflush (fp); // flushing or repositioning required rewind(fp); fgets (mybuffer,80,fp); puts (mybuffer); fclose (fp); return 0; &#125;&#125; 示例 2： 123456789101112131415/*******************************************************************Author: luowei***Description: none****************************************************************/#include &lt;stdio.h&gt;int main()&#123; int num,j,scanf_ret; for (j=0;j&lt;10;j++) &#123; fputs("Input an integer: ", stdout); scanf_ret = scanf("%d", &amp;num); printf("num=%d,scanf_ret=%d\n", num,scanf_ret); &#125; return 0;&#125; 当输入非数字时，比如字母时，那scanf会读取失败，同时输入缓冲区不变，也就是刚输入的字母还在缓冲区中，运行结果：可以用FILE* fp指针，并赋值为stdin，这样可以在调试中观察输入缓冲区的内容。第一次输入之前，stdin缓冲区如下： 第一次输入，输入数字1scanf调用成功，读取数字1： 12Input an integer: 1num=1,scanf_ret=1 此时stdin缓冲区如下，可以发现fp-&gt;_base指向的缓冲区大小为0x1000，目前有三个字符，分别为1\n\n（前两个字节为用户输入的字符，即1和换行符\n，再后面一个\n自己猜测应该是标准库额外加入的，如果输入123，则缓冲区有5个字符，分别为123\n\n），输入函数运行完毕后，fp-&gt;_ptr在fp-&gt;_base后面一字节，表明读取了一个字节给用户，用户输入的换行符和系统加的换行符还在后面。 第二次输入时，输入12345678，scanf调用成功，读取数字12345678： 12Input an integer: 12345678num=12345678,scanf_ret=1 此时fp-&gt;_base指向的缓冲区由1\n\n被覆盖为12345678\n\n，输入函数运行完毕后，fp-&gt;_ptr在fp-&gt;_base后面8字节，表明读取了8个字节给用户，然后转换为数字12345678，用户输入的换行符和系统加的换行符还在后面。 第三次输入，输入b，scanf调用失败，返回值为0，num值不变： 12Input an integer: bnum=12345678,scanf_ret=0 此时，fp-&gt;_base指向的缓冲区由12345678\n\n覆盖为b\n\n45678\n\n，只覆盖了前面的123，因为只输入了b和换行符，加上系统加的换行符，所以前面的123被覆盖为b\n\n，但是fp-&gt;_ptr仍然是fp-&gt;_base，并没有往后推移，表明调用失败，因为b不是一个数字字符。可以看到此时的fp-&gt;_cnt（其指代用户输入的字符数（包括换行符）被成功读取后剩下的数量）不为1，表明用户输入的b和\n被传送至缓冲区后都没有被成功读取，前面成功读取的都为1（剩下用户输入的换行符）. 第三次运行scanf函数时，因为fp-&gt;_ptr指向的并不是\n，换句话说scanf函数的用户态代码还以为内核已经把数据传过来了，于是就直接读取，然后发现b不是数字字符，于是又返回调用失败，num值仍然不变。 1Input an integer: num=12345678,scanf_ret=0 此时，stdin缓冲区不变： 再之后，都是如此。 123456Input an integer: num=12345678,scanf_ret=0Input an integer: num=12345678,scanf_ret=0Input an integer: num=12345678,scanf_ret=0Input an integer: num=12345678,scanf_ret=0Input an integer: num=12345678,scanf_ret=0Input an integer: num=12345678,scanf_ret=0 所以说，当scanf调用失败后，程序员需要去清空输入缓冲区，有的C库实现了fflush(stdin),比如我们在代码中加入： 123456789101112131415161718/*******************************************************************Author: luowei***Description: none****************************************************************/#include &lt;stdio.h&gt;int main()&#123; int num,j,scanf_ret; for (j=0;j&lt;10;j++) &#123; fputs("Input an integer: ", stdout); scanf_ret = scanf("%d", &amp;num); if(scanf_ret==0)&#123; fflush(stdin); &#125; printf("num=%d,scanf_ret=%d\n", num,scanf_ret); &#125; return 0;&#125; 假设之前因为输入b而导致scanf调用失败，此时调用fflush清空输入缓冲区后，效果如下： 可以发现其将fp-&gt;_cnt变为0，下次再调用scanf函数，虽然fp-&gt;_ptr仍然是fp-&gt;_base指向，但是scanf已经可以正常让用户输入而不是直接读取缓冲区。 可以发现，scanf函数在运行时： 先判断需不需要调用读系统调用： 如果fp-&gt;_cnt等于0，或者fp-&gt;_cnt不等于0但是其指向的内容是换行符\n，则调用读系统调用，用户需要输入新的输入，输入的内容（包括输入的换行符）加上库再添加的换行符会覆盖fp-&gt;_base指向的区域，然后fp-&gt;_ptr也会先定位到fp-&gt;_base，然后从fp-&gt;_ptr指向的位置开始读取（库缓冲区到用户态的读取）。 如果fp-&gt;_cnt不等于0且其指向的内容不是换行符\n，则表明缓冲区仍有内容，scanf这时就不去调用读系统调用，也就是不需要用户输入新的输入，而是直接用已有的输入去进行读取处理，从fp-&gt;_ptr指向的位置开始读取（库缓冲区到用户态的读取）。 读成功则fp-&gt;_ptr往后移动，读失败则fp-&gt;_ptr不动，当内容为非数字时，比如字母之类的字符，scanf会返回失败。 当然，由于fflush(stdin)不是所有的C库都支持，所以我们可以自己写代码来完成清空缓冲区的内容，而且是真正的清空，不像fflush(stdin)那样，只是将fp-&gt;_cnt置零，fp-&gt;_ptr不变。代码如下： 1234567891011121314151617181920/*******************************************************************Author: luowei***Description: none****************************************************************/#include &lt;stdio.h&gt;int main()&#123; int num,j,scanf_ret; for (j=0;j&lt;10;j++) &#123; fputs("Input an integer: ", stdout); scanf_ret = scanf("%d", &amp;num); if(scanf_ret==0)&#123; //fflush(stdin); int c; while ( (c=getchar()) != '\n' &amp;&amp; c != EOF ); &#125; printf("num=%d,scanf_ret=%d\n", num,scanf_ret); &#125; return 0;&#125; 如果因为输入错误字符，比如b，则运行完12-16行之间的代码后，效果如下图： 可以看到，由于缓冲区仍有内容，getchar不会调用读系统调用，循环调用getchar读取后，fp-&gt;_ptr指向由b\n\n变为\n，也就是说之前输入的b和换行符已被用getchar函数读走，此时fp-&gt;_cnt为0，这时再运行scanf就可以让用户正常输入了。当然，如果此时再调用getchar，那么getchar就会调用读系统调用了，用于需要输入内容以供getchar读取。这里直接使用gets函数效果与上面完全一样。 总结：getchar、gets、scanf这些输入函数，都会先判断要不要调用读系统调用： 如果fp-&gt;_cnt等于0，则所有读IO函数都需要调用读系统调用。 如果fp-&gt;_cnt不等于0且fp-&gt;_ptr指向的第一个字节是换行符：getchar和gets直接会读取缓冲区，getchar会将换行符返回，而gets会将读取的换行符丢弃；scanf会调用读系统调用让用户输入。 如果fp-&gt;_cnt不等于0且fp-&gt;_ptr指向的第一个字节不是换行符，那三者都不会调用读系统调用，而是直接从输入缓冲区中读内容以作相应处理。 fclose 原型：int fclose ( FILE * stream ); 功能： 关闭stream指定的文件； 所有该stream相关的内部缓冲都会与其解除映射关系并被刷洗：未写入的输出缓冲区内容会被写入，未读的输入缓冲区内容会被丢弃(the content of any unwritten output buffer is written and the content of any unread input buffer is discarded.)； 即使该调用失败，参数指定的stream也不再与其文件及缓冲相映射。 参数：stream：Pointer to a FILE object that identifies the stream. 返回值：如果stream成功关闭则返回零，如果发生错误，则返回EOF。 updated:20150726]]></content>
      <categories>
        <category>内核与系统编程</category>
      </categories>
      <tags>
        <tag>linux kernel</tag>
        <tag>C库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSAPP-虚拟存储器]]></title>
    <url>%2F2015%2F04%2F20%2Fcsapp-virtual-memory%2F</url>
    <content type="text"><![CDATA[关于《深入理解计算机系统》这本书，个人认为这本书的精华是以下三章： 第3章 程序的机器级表示 第5章 优化程序性能 第9章 虚拟存储器 这里谈论第9章的虚拟存储器。 概念理解抽象来说，虚拟存储器被组织为一个存放在磁盘上的N个连续字节单元组成的”数组”，每个字节都有一个唯一的虚拟地址，这个唯一的虚拟地址是作为到”数组”的索引。磁盘上”数组”的内容被缓存在主存中。 虚拟存储器是硬件异常、硬件地址翻译、主存、磁盘文件和内核软件的完美交互，它为每个进程提供了一个大的、一致的和私有的地址空间。它提供了三个重要的能力： 将主存看成是一个存储在磁盘上的地址空间的高速缓存，在主存中只保留活动区域，并根据需要在磁盘和主存之间来回传送数据，通过这种方式，高效的使用主存； 为每个进程提供了一致的地址空间，从而简化了存储器的管理； 保护了每个进程的地址空间不被其他进程破坏。 附：存储器层次结构(memory hierarchy)中的缓存概念 存储器层次结构的本质：每一层存储设备都是较低一层的缓存。 比如本地磁盘作为从远程磁盘取出的文件（比如web页面）的缓存，主存作为本地磁盘的缓存，依此类推，知道最小的缓存–CPU的寄存器集合。 第k+1层的存储器被划分成连续的数据对象片(chunk)，成为块(block)，每个块都有一个唯一的地址或名字。第k层存储器被划分成为较少的块的集合，块大小与k+1层的一样。任何时刻，第k层的缓存都是第k+1层块的一个子集拷贝。数据总是以块大小为传送单元(transfer unit)在第k层和第k+1层之间来会拷贝。 不同层次间的块大小可以不一样。一般而言，层次结构中较低层的设备的访问时间较长，倾向于使用较大的块。寄存器和L1高级缓存之间一般使用1个字的块，L1与L2、L2与L3、L3与主存之间一般使用8-16个字的块，主存与磁盘间一般使用几百或几千字节的块。 在每一层上，我们必须要做缓存的管理，即要将缓存划分成块，在不同的层之间传送块，判定是命中还是不命中，以及相应的处理，尤其是不命中情况下的块替换。 基于缓存的存储器层次结构之所以大行其道，一方面原因是因为较慢的存储设备更便宜；另一方面是因为程序往往体现局部性，包括时间局部性和空间局部性。 原理虚拟存储系统将虚拟存储器分割为成为虚拟页（Virtual Page）的大小固定的块，物理存储器被分割为物理页（Physical Page），两者大小相等，物理页也叫页帧（Page frame）。 任意时间，虚拟页面的集合都分为三个不相交的子集： 未分配的：VM系统还未分配或创建的页，为分配的页没有任何数据和他们相关联，因此也就不占用磁盘空间； 缓存的：当前缓存在物理存储器中的已分配页； 为缓存的：没有缓存在物理存储器中的已分配页。 交换空间一旦一个页面被分配初始化了，它就在一个由内核维护的专门的交换文件（swap file）之间换来换去。交换文件也叫交换空间（swap space）或者交换区域（swap area）。所以，在任何时刻，交换空间都限制着当前运行着的进程能够分配的虚拟页面的综述，也即可用虚拟内存大小等于物理内存大小加交换空间大小。 交换空间通常是一个磁盘分区，但是也可以是一个文件。对于 RAM 小于 1GB 的用户，交换空间通常是推荐的，但是对于拥有大量的物理内存的用户来说是否使用主要看个人口味了(尽管它对于休眠到硬盘支持是必须的)。http://www.linux.com/news/software/applications/8208-all-about-linux-swap-space 参考 CSAPP，第6章：存储器层次结构，第9章：虚拟存储器；]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[彻底理解内存寻址(三)]]></title>
    <url>%2F2014%2F12%2F02%2Fx86-memory-addressing-3%2F</url>
    <content type="text"><![CDATA[x86的段机制把程序的逻辑地址转换成线性地址，这里要讲的分页机制是把线性地址映射成物理地址 1 为什么引入分页16位CPU的内存访问技术所存在的缺点： 单任务： 16位的CPU只支持单任务，也就是同时只有一个程序在运行，随着计算机的发展，单任务的缺点在于体验较差； 内存小： 前面我们知道，在运行程序时，会把程序全部加载到内存中，但是当程序大于内存时，程序就无法运行了； 地址不确定：每次程序装载时分配的地址可能都不一样，使得程序在编写时处理转跳等问题非常麻烦。 安全差： 因为对于内存访问没有太多的限制，所以应用程序很容易去修改操作系统以及BIOS和硬件映射的内存空间，导致系统崩溃； 而当80386引入多任务的支持后，以前的内存管理方式已经不能满足现状的需求的了。于是我们需要新的内存管理方式来解决上面的问题： 地址空间：这个是对物理内存的一个抽象，就好像进程是对CPU的一个抽象。一个进程可用于寻址的一套地址的集合，每个进程都有自己的地址空间，相互独立，这就解决了安全问题。 交换：把程序全部加载到内存，当进程处于空闲时，把他移除内存，存放到硬盘上，然后载入其他程序。这样使得每个进程可以使用更多的内存。 虚拟内存：在老的内存管理中，一次把程序加载到内存，而当程序过大时就无法正常运行了。而利用到计算机系统的局部性和存储分层，我们可以只加载一部分需要使用的代码和数据到内存，当访问的内容不在内存时，和已经使用完的部分进行交换，这样就能在小内存的机器上运行大的程序了。对于程序来说这是透明的，看起来自己好像使用了全部内存。而多个应用完全可以使用相同的虚拟地址。 分段和分页本质不同，16位的分段访问是为了解决地址线位数大于CPU位数的问题。而虚拟寻址则是真正解决了上面提到的那些问题。当然，Intel为了兼容，仍旧支持16位的分段式内存访问。 2 相关软硬件概念 实现分页机制的硬件结构有： 四个专用于分页机制的32位寄存器：CR0、CR1、CR2、CR3； 分页部件； 页面高速缓存； 软件结构有： 页 页表 页目录 页表项。 分页专用寄存器CR0中包含了6个预定义标志，这里只介绍内核中用到的0位和31位。第0位是保护允许位， 为1时启动保护模式。31位用于分页允许，为1时表示开启分页机制，为0时表示不用分页机制，这时只用段机制实现地址映射。。 CR1暂时没用。 CR2是缺页线性地址寄存器，当发生缺页异常的时候，该寄存器保存最后一次出现缺页的全32位线性地址，这在异常处理时会介绍到，这里我们不深究它的实现过程。 CR3叫做页目录基址寄存器，顾名思义，它保存的是页目录的物理地址。我们就是通过该寄存器里边的值，找到对应的页目录中的页表项，从而知道我们的一个页是存储里内存中的哪个位置的。 2.1 页分页机制的思想是将线性地址空间划分成若干相等的片，这一片就称为一个页，并从0开始给各页编号。相应的，我们也把物理地址空间划分成与页大小相等的若干存储块，也同样为他们编号，仍然从0开始。 这样就有一个问题了，因为页是固定大小的，那么它过大或者过小都会影响内存使用率。页过大，有时候会存在冗余空间；页过小，若有分支指令，则会存在在页之间跳转的情况，同样影响效率。x86支持的标准页大小为4KB（也支持4MB）。 2.2 页表页表是把线性地址转换成物理地址的一种数据结构。有点类似于段描述符表的作用。它包括两个成员： 物理页面基地址：线性地址空间中的一个页装入内存后所对应的物理页面的起始地址。 页的属性：描述一些页的属性信息。 页面大小为4KB，即一个页面会占4KB的空间，那么每个页面的物理页面基地址必然是4KB的整数倍，也就是说其地址的最低12位总是0，那么我们就可以用这12位存放页的属性，这样用32位的地址就完全可以描述页的映射关系，也就是页表中一个表项占四个字节就够了。 不过，4GB的线性地址空间可以被划分为1M个4KB大小的页，每个页表项占4个字节，则1M个页表项就需要占4MB的空间，而且还要求是连续的，这显然是不现实的，于是把这4MB的页表再以4KB为大小分页，分为1K个页，同样对每个项描述需要四个字节，这就是两级页表的管理方式。 2.2.1 两级页表两级页表的第一级把它叫做页目录，用它来管理1M个页的页表。上边说我们把1M个页表项以4KB为页分了1K个页，于是我们用10位就可以检索到这1K个页目录的每个目录项了。现在我们把这1K个页目录（占4KB）放进内存的某个位置，这个位置就叫做页目录起始地址。我们把这个起始地址放进CR3中，现在来一个线性地址，我们根据线性地址的前10位，再根据CR3中的页目录起始地址，就可以得到目录项在内存中的地址。那我们读取这个目录项，其高20位就是页表在内存中的起始地址。于是我们用线性地址的中间10位和这个页表起始地址就可以计算出页表项的位置，这个页表项和线性地址的的低12位就可以把这个线性地址定位到内存中具体的位置了。 页面高速缓存由于在分页情况下，页表是放在内存中的，这使CPU每次至少两次去访问内存。页面高速缓冲器保存最近处理过的32项页表项。当访问线性地址空间的某个地址时，先检查对应的页表项是否在缓存中，如果在就没必要两次访问内存了。高速缓存的命中率是相当高的。 3 页地址转换4 段页式地址转换分段内存管理的优势在于内存共享和安全控制，而分页内存管理的优势在于提高内存利用率。他们之间并不是相互对立的竞争关系，而是可以相互补充的。也就是可以把2种方式结合起来，也就是目前计算机中最普遍采用的段页式内存管理。段页式管理的核心就是对内存进行分段，对每个段进行分页。这样在拥有了分段的优势的同时，可以更加合理的使用内存的物理页。 上面的图简单的描述了在段页式内存管理的系统中地址转换的过程。 CPU给出要访问的逻辑地址； 通过分段内存管理的地址转换机制，将逻辑地址转换为线性地址，也就是分页系统中的虚拟地址； 通过分页内存管理的地址转换机制，将虚拟地址转换为物理地址； 目前主流的系统中都采用了段页相结合的内存管理方式，当然不同的系统具体实现起来是不同的。Linux操作系统在386保护模式下把段基址设为0，段长度设为4GiB，从而模拟了平面内存模型。所有段首地址都是0x00000000，所以逻辑地址和转换得到的线性地址是一致的，即逻辑地址的偏移量字段的值与线性地址的值总是相同的。。 《现代操作系统》3.7章节关于分页与分段的比较: 待参考http://blog.csdn.net/yusiguyuan/article/details/12746251http://ccq.qijuwang.cn/n/t/b_3.htmlhttp://www.cs.virginia.edu/~cs333/notes/virtualmemory.pdfhttp://www.360doc.com/content/12/0920/15/7982302_237218174.shtmlhttp://www.motherboardpoint.com/threads/l1-l2-caches-and-mmu.152345/http://www.aliog.com/3979.htmlhttp://wiki.0xffffff.org/posts/hurlex-9.htmlhttp://www.adintr.com/article/blog/298http://www.cnblogs.com/xkfz007/archive/2012/10/08/2715163.htmlhttp://blog.csdn.net/drshenlei/article/details/4582197非常棒的一组文章http://blog.csdn.net/drshenlei/article/category/551407]]></content>
      <categories>
        <category>计算机底层</category>
      </categories>
      <tags>
        <tag>X86内存寻址</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[彻底理解内存寻址(一)]]></title>
    <url>%2F2014%2F12%2F01%2Fx86-memory-addressing-1%2F</url>
    <content type="text"><![CDATA[X86是一种比较特殊的处理器体系架构，也是现今计算机最常见的处理器架构。 1 内存模型1.1 flat memory model wiki/Flat_memory_model 即linear memory model，这种存储器定址方式(memory addressing paradigm)下，应用程序看到的内存是一个单独的连续地址空间，CPU可以直接（且线性）地寻址所有可利用的内存位置，无需借助任何内存分段(memory segmentation)或分页机制(paging schemes)。 1.2 Paged memory model1.3 X86 Segmented memory model2 x86 memory segmentation wiki/Memory_segmentationwiki/X86_memory_segmentation,中文页面 x86 memory segmentation是X86指令集架构下内存分段(memory segmentation)的实现。 1978年Intel 8086中首次引入分段(segmentation)，这使得16位CPU可以访问超过64 KB (2^16，65,536字节)的内存，8086处理器与内存芯片间的地址总线是20位，即可访问2^20=1MB内存。 1982年，Intel 80286则引入了另一种分段，其支持内存保护，并将8086的模式命名为real mode，新的模式则叫做protected mode。 从8086开始到随后的各款x86架构CPU，无论是实模式还是保护模式，内存寻址时都使用16位段寄存器（segment register)，但两种模式下对段寄存器的使用是不一样的。 处理器启动时处于real mode，即使用8086的分段寻址模式。 1985年Intel 80386以及之后的X86架构CPU的protected mode都保持了80286 protected mode下的分段机制。增加了virtual 8086 mode，也叫virtual real mode, V86-mode，当处理器运行在protected mode的操作系统下，可以直接运行real mode的程序，这是一种硬件虚拟化技术。 80386在segmentation unit和physical bus之间加入了paging unit，作为第二级的地址翻译，集成了MMU来完成这一功能。 除了CS, DS, ES和SS，增加了两个段寄存器FS和GS，这两个寄存器并无硬件绑定的用途；另外与80286不同的是，80386只需要将CRO控制寄存器的某个位清零就可以进入real mode。 2003年的x86-64架构在long mode(64-bit mode)下不再使用分段(segmentation)。四个段寄存器CS, SS, DS, ES强制为0，段长度强制为2^64。段寄存器FS、GS可以有非0值，被操作系统用于其它用途。上电后，CPU处于real mode且与32-bit Pentium IV几乎一样，只有在long mode开启后64-bit指令才可以运行，而且CPU必须先进入32-bit protected mode然后才能进long mode。当处于long mode时，16-bit的指令以及virtual x86 mode无效，protected mode也不再工作。 段的选择通常由处理器根据正在被执行的指令默认指定。指令总是从code segment里面获取，对任何栈的push或pop操作以及栈中的数据引用都是使用stack segment，其他数据的引用使用data segment，extra segment默认用来string operation(如MOVS、CMPS)，FS和GS并没有硬件绑定的用途。处理器指令中可以明示使用哪些段寄存器，这将替换掉默认使用的段寄存器。 3 X86各模式3.1 实模式(real mode)实模式与虚拟86模式下，一个段总是长64,536字节（16位段内偏移地址）。在段寄存器内的16-bit被解释为20位线性地址空间的高16位，称为段地址；其余的低4位全为0。段地址与16位段内偏移地址相加产生线性地址，同时也是这种内存模式下的物理地址.任何程序都可以访问全部内存空间。没有对内存的访问权限保护。 这是Intel分段设计的根源，寄存器和数据总线位宽为16位，而地址位宽却是20位，16位的数据位宽只能表示64KB内存，而20位的地址位宽可实际寻址1MB。于是引入了段寄存器（segment register），用来告诉CPU一条程序指令将操作哪一个64K的内存区块。先加载段寄存器，相当于表明打算操作开始于X处的内存区块，之后再用16位的内存地址来表示相对于那个内存区块（或段）的偏移量。总共有4个段寄存器：一个用于栈（ss），一个用于程序代码（cs），两个用于数据（ds，es）。在那个年代，大部分程序的栈、代码、数据都可以塞进对应的段中，每段 64KB 长，分段功能经常是透明的。 之后，分段功能就一直存在了。 当CPU执行指令需要访问内存时，只会送出段内的偏移地址，而通过指令的类型类确定访问哪一个段寄存器。 实模式的段地址以16个字节为步长，从0开始编号一直到 0xFFFF0 （即 1MB ）。你可以将一个从0到0xFFFF的16位偏移量（逻辑地址）加在段地址上。在这个规则下，对于同一个内存地址，会有多个段地址/偏移量的组合与之对应，而且物理地址可以超过1MB的边界，只要你的段地址足够高（参见臭名昭著的A20线）。在实模式的C语言代码中，一个远指针（far pointer）既包含了段选择符又包含了逻辑地址，用于寻址1MB的内存范围。 随着程序变得越来越大，超出了64K的段，分段功能以及它古怪的处理方式，使得x86平台的软件开发变得非常复杂。Intel又引入了新的分段机制，仍然使用16位的段寄存器，但其表示的意义不再一样。 3.2 80286保护模式Intel 80286处理器引入的保护模式是另一种分段机制，仍然使用16位段寄存器与16位的段内偏移地址，但支持访问2^24（16M）字节的内存。16位段寄存器内不再是段地址，其高13位被称作段选择符（segment selector），值是到段描述符表(segment descriptor中)的索引值。段描述符中包含了24位的segment base，20位的段长度。段开始地址与段内偏移地址相加为线性地址，并作为内存物理地址。段的长度上限为2^20=1M字节。 3.3 80386保护模式Intel 80386处理器继续使用80286的分段保护模式，仍然使用16位段寄存器，但段内偏移地址和段描述符中的segment base为32位。 在分段转址与物理地址之间又增加了一层分页（paging）转址。分段寻址是不能关闭的,分页可以使能或关闭，如果关闭就与80286保护模式一样。如果使用分页机制，则由段开始基地址与段内偏移地址相加得到的是线性地址（虚地址），线性地址还需要由paging unit翻译成物理地址。80386下的物理地址是32-bit，但是在之后支持PAE的处理器下可以超过32-bit。 这里，我们可以发现，由于段基址和偏移地址都是32位的，所以不管怎样，寄存器和指令都可以寻址整个线性地址空间，所以根本就不需要再去使用基地址。完全可以将基地址设成0，让逻辑地址与线性地址一致呢。Intel的文档将之称为“扁平模型”（flat model），而且在现代的x86系统内核中就是这么做的（特别指出，它们使用的是基本扁平模型）。基本扁平模型（basic flat model）等价于在转换地址时关闭了分段功能，注意是等价于，并不是真的关闭，因为也关闭不了。 可以看到，Intel巧妙的绕过了他们原先设计的那个拼拼凑凑的分段方法，而是提供了一种富于弹性的方式来让我们选择是使用段还是使用扁平模型。由于很容易将逻辑地址与线性地址合二为一，于是这成为了标准，比如现在在 64 位模式中就强制使用扁平的线性地址空间了。但是即使是在扁平模型中，段对于x86的保护机制也十分重要。保护机制用于抵御用户模式进程对系统内核的非法内存访问，或各个进程之间的非法内存访问，详见下一篇文章。 4 地址 物理地址在支持Intel的主板芯片组上，CPU对内存的访问是通过连接着CPU和北桥芯片的前端总线来完成的。在前端总线上传输的内存地址都是物理内存地址 ，编号从0开始一直到可用物理内存的最高端。这些数字被北桥映射到实际的内存条上。物理地址是明确的、最终用在总线上的编号，不必转换，不必分页，也没有特权级检查。 逻辑地址在 CPU 内部，程序所使用的是逻辑内存地址，它必须被转换成物理地址后，才能用于实际内存访问。逻辑地址指的是机器语言指令中，用来指定一个操作数或者是一条指令的地址。Intel为了兼容，将远古时代的段式内存管理方式保留了下来。Intel段式管理中，对逻辑地址要求，“一个逻辑地址，是由一个段标识符加上一个指定段内相对地址的偏移量，表示为 [段标识符：段内偏移量]”。 线性地址跟逻辑地址类似，它也是一个不真实的地址，如果逻辑地址是对应的硬件平台段式管理转换前地址的话，那么线性地址则对应了硬件页式内存的转换前地址。 虚拟地址分页机制下的线性地址。 上图描述了在CPU的分页功能开启的情况下内存地址的转换过程。如果CPU关闭了分页功能，或运行于16位实模式，那么从分段单元（segmentation unit ）输出的就是最终的物理地址了。当CPU要执行一条引用了内存地址的指令时，转换过程就开始了。 CPU地址转换需要进行两步：首先给定一个逻辑地址（即段内偏移量），CPU要利用其段式内存管理单元，先将每个逻辑地址转换成一个线性地址，再利用其页式内存管理单元，转换为最终物理地址。 这样做两次转换，的确是非常麻烦而且没有必要的，前面已经提过一次了，因为直接可以把线性地址抽象给进程。之所以这样冗余，Intel完全是为了兼容而已，见8086设计。硬件要求这样做了，软件就只能照办。而Linux的段式管理，事实上只是“哄骗”了一下硬件而已。 参考为什么x86上Linux kernel没有自己独立的地址空间？linux内存管理—虚拟地址、逻辑地址、线性地址、物理地址的区别（一）我理解的逻辑地址、线性地址、物理地址和虚拟地址计算机原理学习（7）– x86-32 CPU和内存管理之分段管理http://www.kerneltravel.net/books/ulk_ch02.pdf]]></content>
      <categories>
        <category>计算机底层</category>
      </categories>
      <tags>
        <tag>X86内存寻址</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[彻底理解内存寻址(二)]]></title>
    <url>%2F2014%2F12%2F01%2Fx86-memory-addressing-2%2F</url>
    <content type="text"><![CDATA[x86的分段机制是实现程序的逻辑地址到线性地址的映射的一种机制。 本文以80386保护模式为主。 1 一些概念 硬件：16位的段寄存器、分段部件、段描述符高速缓冲器； 软件(数据结构)：段描述符表、段描述符； 两个概念：逻辑地址和线性地址。上一篇文章已提到。 1.1 段寄存器1.1.1 8086实模式前面一篇文章已经提到，这里就不说了，以80286/80386为主。 1.1.2 80386保护模式逻辑地址是16位的段选择符:32位偏移地址，段寄存器不再保存段基址，取而代之的是一个索引编号，用于引用段描述符表中的表项。这个表为一个简单的数组，元素长度为8字节，每个元素描述一个段。 80286和80386的分段机制中，段寄存器存放的就是段描述符在段描述符表中的索引，严格来说是前13位存储段描述符的索引，然后第2位(TI)表示是从全局描述符表(GDT)中选择段描述符还是从局部描述符表（LDT）中选择段描述符。剩下两位表示请求者的特权级。 之所以叫保护模式，就是因为保护模式提供了CPU的不同运行特权，ss（堆栈段寄存器）和ds（数据段寄存器）。这些内容里包含了请求特权级（Requested Privilege Level，简称RPL）字段，用0-3表示。0表示最高特权级，对应内核态，此时它可以访问内核代码，也可以访问用户代码；3表示最低特权级，对应用户态，此时它只能访问用户态代码。cs(代码段寄存器)拥有一个由CPU自己维护的当前特权级字段（Current Privilege Level，简称CPL），这点对我们来说非常重要。代码段寄存器中的2位宽的CPL字段的值总是等于CPU的当前特权级。不管CPU内部正在发生什么，只要看一眼cs中的CPL，你就可以知道此刻的特权级了。 1.1.3 使用分段功能自8086提出后，一直被x86处理器所使用着。每一条会访问内存的指令都隐式的使用了段寄存器。比如，一条跳转指令会用到代码段寄存器(cs)，一条压栈指令(stack push instruction)会使用到堆栈段寄存器（ss）。 在大部分情况下你可以使用指令明确的改写段寄存器的值。段寄存器存储了一个16位的段选择符（segment selector）,它们可以经由机器指令（比如MOV）被直接加载。唯一的例外是代码段寄存器（cs），它只能被影响程序执行顺序的指令所改变，比如CALL或JMP指令。虽然分段功能一直是开启的，但其在实模式与保护模式下的运作方式并不相同的。 1.1.4 内存保护CPU会在两个关键点上保护内存：当一个段选择符被加载时，以及，当通过线形地址访问一个内存页时。因此，保护也反映在内存地址转换的过程之中，既包括分段又包括分页。当一个数据段选择符被加载时，就会发生下述的检测过程： 因为越高的数值代表越低的特权，上图中的MAX()用于挑出CPL和RPL中特权最低的一个，并与描述符特权级（descriptor privilege level，简称DPL）比较。如果DPL的值大于等于它，那么这个访问就获得许可了。RPL背后的设计思想是：允许内核代码加载特权较低的段。但堆栈段寄存器是个例外，它要求CPL，RPL和DPL这3个值必须完全一致，才可以被加载。 事实上，段保护功能几乎没什么用，因为现代的内核使用扁平的地址空间。在那里，用户模式的段可以访问整个线性地址空间。真正有用的内存保护发生在分页单元中，即从线性地址转化为物理地址的时候。一个内存页就是由一个页表项（page table entry）所描述的字节块。页表项包含两个与保护有关的字段：一个超级用户标志（supervisor flag），一个读写标志（read/write flag）。超级用户标志是内核所使用的重要的x86内存保护机制。当它开启时，内存页就不能被ring3访问了。尽管读写标志对于实施特权控制并不像前者那么重要，但它依然十分有用。当一个进程被加载后，那些存储了二进制镜像（即代码）的内存页就被标记为只读了，从而可以捕获一些指针错误，比如程序企图通过此指针来写这些内存页。这个标志还被用于在调用fork创建Unix子进程时，实现写时拷贝功能（copy on write）。 关于CPU特权级切换的详细使用参考：CPU的运行环、特权级与保护 1.2 段描述符一个程序可以分解成多个在逻辑上相对独立的模块。每个模块作为一个单独的段，都以该段的起点进行相对编址。所以我们需要用一个数据结构描述各个段的相关信息，比如该段装进内存的基址（段基址），该段的长度，以及该段是否存在内存中等信息，这个数据结构就是段描述符。这样，每个模块都有一个段描述符，用来描述该段的属性，而段描述符表就是段描述符的集合。 段描述符表的表项就是段描述符，段描述要存储三个信息：基地址、段长度和段属性。用8个字节来存储这三个信息，段基址是32位所以占四个字节，然后用20位来描述段长度，剩下12位就用来存储段的属性信息了。 参考：http://book.51cto.com/art/201006/208522.htmhttp://www.cppblog.com/xingkongyun/articles/62743.html 1.2.1 80286描述符有八字节，在80286中，组内有最高2个字节保留未用，其余6个字节分别存储着程序（或数据）段所占内存的长度（段限长，2个字节）、起始的物理地址（称段基地址，3个字节）和访问权（1个字节）。内存中每段程序（或数据）均可用一个描述符来表示。 1.2.2 80386段属性信息80386的段描述符内容比80286要丰富很多。 G：一位，G=0：表示以字节为单位表示段的长度，刚才我们说了我们用20位存储段的长度，那么这样我们段的最大长度就是2^20次方，即1M大小，即我们给程序分段的时候，一个段最大是1M。G=1:表示以4KB表示段的长度，这样我们一个段的最大长度就是2^20次方乘以4KB，即4GB。后边我们会知道linux内核就让这个G等于1，从而直接越过x86的段机制，使逻辑地址不用映射线性地址，二是直接映射到物理地址(分页机制)。 D:一位，表示操作数的位数，D=1表示32位操作数，D=0表示16位操作数，这显然是为向下兼容而设计的。 接下来两位暂时没用，可留作扩展。 P：一位，表示这个段是否在内存中，如果我们要把这个段加载进内存，那么在加载进去的时候把这个值修改为1，否则让它等于0。 DPL：两位，表示段描述符的特权级。其值从0（最高特权，内核模式）到3（最低特权，用户模式），用于控制对段的访问。 S：一位，表示这个段是系统段还是用户段。S=0，则为系统段，即内核专门使用的段，S=1,表示用户段，即为程序的代码段、数据段或堆栈段。 类型占三位：依次是E、D、W。E=0，为数据段描述符，这时D位表示数据的扩展方向，D=0,表示向地址增大的方向扩展，反之，向地址减小的方向扩展。E=1时，直接表示的是数据段，此时W=0时表示数据段不能写（我们就可以想到C++中定义const变量的时候，存储const变量的段肯定是修改了这个值的），W=1,数据段可写。 保护模式下，有三种类型的描述符表，分别是全局描述符表（GDT），中段描述符表（IDT），局部描述符表（LDT）。为了加快对这些表的访问，Inter设计了三个专门的寄存器，GDTR、IDTR、LDTT，以存放这些表的基地址和表的长度界限。 1.3 段描述符表将不同程序（或数据）段的描述符在内存中连续排列起来，所形成的表称为描述符表。 由与全局有关的程序（或数据）段的描述符组织在一块的表称全局描述符表（GDT）。一个系统内只有一张GDT。存放该表的物理地址和长度的寄存器称GDTR。 所有中断服务程序的描述符表称中断描述符表（IDT），一个系统也仅有一张IDT，中断描述符表是CPU用来处理中断和程序异常的，中断描述符表中存储的是门描述符。每一个中断都被赋予一个从0到255的编号，叫做中断向量。处理器把中断向量作为IDT表项的索引，用来指出当中断发生时使用哪一个门描述符来处理中断。 由不涉及全局的程序（或数据）段的描述符所组成的表称局部描述符表（LDT）。 GDT和LDT各占用64KB内存，含8K个描述符。 由于一个段描述符是8字节长，因此它在GDT或LDT内的相对地址是由段选择符的最高13位的值乘以8得到的。 例：如果GDT在0x00020000（这个值保存在gdtr中），且由段选择符所指定的索引号为2，那么相应的段描述符地址是0x00020000 + （2 * 8）= 0x00020010 1.4 段描述符高速缓冲寄存器为了避免在每次存储器访问时，都要访问描述符表而获得对应的段描述符，从80286开始每个段寄存器都配有一个高速缓冲寄存器，称之为段描述符高速缓冲寄存器或描述符投影寄存器，对程序员而言它是不可见的，即每个段寄存器都有一个隐藏部分用于缓存段选择符所对应的那个段描述符。 段描述符的内容一旦被访问，就会被cache，所以在随后的访问中，就不再需要去实际读取GDT了，否则会有损性能。即，每当把一个选择子装入到某个段寄存器时，处理器从描述符表中取出相应的描述符，把描述符中的信息保存到对应的高速缓冲寄存器中。此后对该段访问时，处理器都使用对应高速缓冲寄存器中的描述符信息，而不用再从描述符表中取描述符。 段描述符高速缓冲寄存器之内保存的描述符信息将一直保存到重新把选择子装载到段寄存器时再更新。程序员尽管不可见段描述符高速缓冲寄存器，但必须注意到它的存在和它的上述更新时机。例如，在改变了描述符表中的某个当前段的描述符后，也要更新对应的段描述符高速缓冲寄存器的内容，即使段选择子未作改变，这可通过重新装载段寄存器实现。 参考：http://wenku.baidu.com/link?url=-eUpCYmSxHwy3CnJCpgy08_xNZ-gR3j8SswY5_lscN6hSDm6HzjQWZB8HfFJV70QAaJoXlo9m0dHr6AozjwlBiMBVY0L91iR2g48DmqYjGK 2 段地址转换 下图更详细一点： 流程顺序描述如下： 首选确定要访问的段（方式与x86-16相同），然后决定使用的段寄存器。 根据段选择符号的TI字段决定是访问GDT还是LDT，他们的首地址则通过GTDR和LDTR来获得。 将段选择符的Index字段的值*8，然后加上GDT或LDT的首地址，就能得到当前段描述符的地址。（乘以8是因为段描述符为8字节） 得到段描述符的地址后，可以通过段描述符中BASE获得段的首地址。 将逻辑地址中32位的偏移地址和段首地址相加就可以得到实际要访问的物理地址。 3 linux的使用与其说linux是如何利用X86中的分段机制，还不如说是如何绕过linux分段机制。 linux是一个可移植的操作系统，它支持x86，Alpha，arm等多种体系结构。但是Alpha，arm很多体系结构其实是不支持分段机制的，但是他们都支持分页机制。linux为了能移植到x86上，做了不少工作。 首先要在x86上运行程序，那不可避免要用到段机制，因为X86的分段机制不可关闭。80386的段描述符中有一个表示以字节为单位还是以4KB为单位表示一个段长度的属性位。以4KB为单位，那么一个段最大长度能到4GB。根据这一点，我们把一个段的段基址固定设置为0，然后让G=1，于是我们一个段的最大长度就是4GB了，这个就能和我们4GB的线性地址空间一一映射了。通过这样的处理，就跳过了x86的段机制，逻辑地址和线性地址可以混为一谈了。 但是x86分段的好处还是要用起来的，比如必须为代码段和数据段创建不同的段，这样linux为代码段和数据段分别创建了一个基地址为0，段长度为4GB的段描述符。不仅如此，由于linux内核运行在特权级0，用户程序运行在特权级3，x86规定说特权级为3的用户程序是不能访问特权级为0的内核代码的，所以linux又分别为内核和用户程序分别创建代码段和数据段。 于是在arch/x86/include/asm/segment.h中这样定义四个段（即在机器启动过程中段寄存器中放的值）： 1234#define __KERNEL_CS (GDT_ENTRY_KERNEL_CS*8) #define __KERNEL_DS (GDT_ENTRY_KERNEL_DS*8) #define __USER_DS (GDT_ENTRY_DEFAULT_USER_DS*8+3) #define __USER_CS (GDT_ENTRY_DEFAULT_USER_CS*8+3) 其中： 12345#define GDT_ENTRY_DEFAULT_USER_CS 14 #define GDT_ENTRY_DEFAULT_USER_DS 15 #define GDT_ENTRY_KERNEL_BASE (12) #define GDT_ENTRY_KERNEL_CS (GDT_ENTRY_KERNEL_BASE+0) #define GDT_ENTRY_KERNEL_DS (GDT_ENTRY_KERNEL_BASE+1) 于是上边的定义的结果是下边这样： 1234#define __KERNEL_CS 0x00C0 /*内核代码段，index=12，TI=0,RPL=0*/ #define __KERNEL_DS 0x00D0 /*内核数据段，index=13，TI=0,RPL=0*/ #define __USER_DS 0x00E3 /*用户数据段，index=14，TI=0,RPL=3*/ #define __USER_CS 0x00F3 /*用户代码段，index=15，TI=0,RPL=3*/ 于是我们可以用12,13,14,15四个索引来找到我们四个段所对应的段描述符，并且我们把内核代码段的特权声明为0，用户代码段的特权为3，TI为0表示我们总是访问全局描述符表。 在我们对应的段描述符中我们把G设置为1，段上限规定为0xfffff，就巧妙的绕过了x86的分段机制。 来看看Linux 用户模式应用程序的一个跳转指令： 但在这里我不能忽略的一个问题就是，我们把四个段的上限全部设置为4G，那就完全破坏了段的保护，就是说，我们有可能随随便便就修改了我们的其他段的数据。 但是现在还是个线性地址，我们还是有处理这个问题的办法，这就是80386中同时引入的分页机制，即在线性地址与物理地址之间来一层映射。 4 参考：http://blog.csdn.net/bcs_01/article/category/1409609http://www.ahlinux.com/start/kernel/6876.html]]></content>
      <categories>
        <category>计算机底层</category>
      </categories>
      <tags>
        <tag>X86内存寻址</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sshfs使用-windows上远程访问linux端目录]]></title>
    <url>%2F2014%2F11%2F20%2Fsshfs%2F</url>
    <content type="text"><![CDATA[sshfs，顾名思义，是基于SSH文件传输协议 (SSH File Transfer Protocol，也称 Secure File Transfer Protocol，Secure FTP或SFTP)的文件系统，sshfs的实现基于fuse，sshfs的作者也是fuse的作者。 sshfs官网 sftp与ftp有着几乎一样的语法和功能。SFTP为 SSH的一部份，一般来说ssh server都会包含SFTP，比如在Ubuntu下安装ssh server时： 123456789101112$ sudo apt-get install openssh-serverReading package lists... DoneBuilding dependency tree Reading state information... DoneThe following extra packages will be installed: libck-connector0 ncurses-term openssh-sftp-server python-requests python-urllib3 ssh-import-idSuggested packages: rssh molly-guard monkeysphereThe following NEW packages will be installed: libck-connector0 ncurses-term openssh-server openssh-sftp-server python-requests python-urllib3 ssh-import-id 可以看到openssh-sftp-server这个包也安装了。 SFTP本身没有单独的守护进程，它必须使用sshd守护进程（端口号默认是22，可/etc/ssh/sshd_config中配置）来完成相应的连接操作，所以从某种意义上来说，SFTP并不像一个服务器程序，而更像是一个客户端程序。SFTP使用加密传输认证信息和传输的数据，是非常安全的。但是由于这种传输方式使用了加密/解密技术，所以传输效率比普通的FTP要低得多，如果您对网络安全性要求更高时，可以使用SFTP代替FTP。 linuxserver端开启ssh server后什么都不用做，client端挂载文件系统即可，就像ssh登录一样简单。 挂载远程主机目录到本地目录： 1$ sshfs user@hostname:path mount_point 需要输密码就输密码，当然配置成自动登录的ssh更方便。 用完之后要umount，可以： 1$ fusermount -u mount_point windows win-sshfs, SSHFS client for Windows win-sshfs, GitHub repo 安装 其依赖Microsoft .NET Framework 4和Dokan Library 0.6.0，安装时会检测，若没有Dokan库会自动安装。 MAC在 Mac OS X 上使用 SSHFS可以用FUSE for OS X： FUSE for OS X, Github Project 参考：http://blogger.gtwang.org/2014/05/sshfs-ssh-linux-windows-mac-os-x.html]]></content>
      <categories>
        <category>tools使用</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从为什么计算机要用补码来表示数值谈开去]]></title>
    <url>%2F2014%2F11%2F18%2Fwhy-two's-complement%2F</url>
    <content type="text"><![CDATA[原码就是符号位加上真值的绝对值, 即用第一位表示符号, 其余位表示值。 正数的反码是其本身，负数的反码是在其原码的基础上, 符号位不变，其余各个位取反。 正数的补码就是其本身，负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1. (即在反码的基础上+1)。 这些是很多人为了应付考试而背诵的，但是，背后的原因，还是应该去探究探究的！ 机器数和真值真值就是我们真实世界里的数字表示方法，正数1就表示为1（严谨点为：+1），负数1就表示为-1。但是计算机不能这么认为，对，computer is stupid！计算机内部得有个约定来表示数值，尤其是符号位。所以，这就涉及到了编码，“编”这个字就表明这是人为规定的，换句话说就是机器数是我们对数编码之后的二进制表示形式。 所以，问题来了！ 我们该采用什么样的编码规则？而且很明显我们想达到这样的目的：在此编码下，机器数经过加法器运算得到的机器数，其在我们的编码规则下正好是我们真实世界里的数值运算结果。而补码这一编码规则来表示有符号数，可以使加法器的设计异常简单。 一句话点名补码背后的数学原理就是取模运算[注1]，加上加法器的会让溢出的位丢弃，这样就使得补码可以满足上述要求。 用现实生活中来解释这个最好的例子是钟表，用十二小时制来想象一个钟表，假设现在是二点钟，我要到一点钟，很自然我可以往后拨1格，2-1=1，我也可以往前拨13格，2+11=13，但是因为中间过了个12,就要变为1，也就是说钟表会把12丢掉（这不就是十二进制的加法运算，最高位溢出，然后将溢出位丢弃，因为13表示不出来啊！！！而十二进制丢弃一位不就是丢弃十进制的12嘛，），也就是说2+11在钟表的法则下等于1，也就是说，这里，2与-1相加和2与11相加结果都是1。在钟表法则（只有1位数的十二进制数）下，-1等于11啊！！！ 到这里，我们知道计算机里面所能表示的数值范围是有限的，加法器会把超出CPU字长的位丢弃（比如32位的CPU，运算时会把溢出的第33位丢弃），所以，我们应该想到，要想达到有符号数的正确运算效果，我们必须像钟表那样，采取某种编码方式，让“-1等于11“。 也就是说，补码编码方式得满足：（为讨论方便，我们假设CPU字长为8） 0000 0001+codeone=0（因为(+1)+(-1)=0），0000 0001为+1的补码，codeone为-1的补码； 因为超过八位的位机器会将其会溢出丢弃，所以很明显想到codeone为1111 1111，因为其加0000 0001后结果为1 0000 0000，但8位计算机只能表示8位，所以在计算机看来结果就是0了。所以，-1的补码就是11111111。 让我们用书上写的那条规则再来算一下，”负数的补码是原码基础上，符号位不变，其余位取反，然后加1（即反码基础上加1）“： -1的原码为1000 0001 （可以看出原码是最符合人类思维，最高位为符号位，其余位为绝对值，一眼知其真实数值，可惜它不能用于电脑运算）-1的反码为1111 1110 （反码就不太友好了，你不能一眼看出其真实数值，它也不能用于电脑运算）-1的补码为1111 1111 （所以，补码虽然对人类不友好，但它对计算机友好啊！）与上述一致！ Bits 无符号数 补码 0111 1111 127 127 0111 1110 126 126 0000 0010 2 2 0000 0001 1 1 0000 0000 0 0 1111 1111 255 −1 1111 1110 254 −2 1000 0010 130 −126 1000 0001 129 −127 1000 0000 128 −128 Appendix 1在理解了上面的文字之后，要看详细内容，可以参考下面的这个博文(猛戳)，此文写的很详细且清晰易懂！ Appendix 2看完了上面这个博客，我觉得还有必要理解一下英文术语以及港台朋友们对此的术语：Signed number representations,点击查看有符号数表示，中文链接 one’s complement,点击查看大陆翻译作反码，港台称作一补数，看得出来港台原汁原味啊，中文wiki上”补码“被重定向至”一补数“,中文链接 two’s complement,点击查看大陆翻译作补码，港台称作二补数,中文链接 [注1]取模运算和取余运算是不一样的。各个环境下%运算符的含义不同，比如c/c++，java 为取余，而python则为取模。详情戳此]]></content>
      <categories>
        <category>root explanation</category>
      </categories>
      <tags>
        <tag>计算机系统</tag>
      </tags>
  </entry>
</search>
